{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hansard Analysis\n",
    "\n",
    "This notebook contains code to analyse Hansard, the official record of every spoken or written contribution made in the Houses of Parliament from 1803 to the present day.\n",
    "\n",
    "*Contains Parliamentary information licensed under the Open Parliament Licence v3.0.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why bother?\n",
    "\n",
    "Looking at changes in political discourse over time is fascinating. We can infer a lot about which issues were prioritised by different governments, how Members of Parliament responded to global events, when emerging technologies were first mentioned in Parliament, and how the tone surrounding different issues has changed.\n",
    "\n",
    "![Hansard](https://assets3.parliament.uk/iv/main-large//ImageVault/Images/id_10860/scope_0/ImageVaultHandler.aspx.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the archives\n",
    "\n",
    "The online Hansard archive (available at https://hansard.parliament.uk/) is an incredibly rich source of information, and is free to access. It's also a bit of a mess; this means it's more efficient to download a local copy of Hansard and search that, so we'll start off by writing some code to automatically download one HTML file for every day's worth of Parliamentary debate.\n",
    "\n",
    "Downloading all this will take a good couple of days even with the code running continuously, but on the plus side your non-technical friends will think you're orders of magnitude more intelligent than you actually are if you intersperse every conversation with \"sorry, I just need to go and check my code is still running.\" For bonus points, switch the colour scheme on your terminal to green on black and be sure to accidentally on purpose give them a glimpse of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2 # requests and grabs information from web pages\n",
    "from bs4 import BeautifulSoup # parses and searches HTML files\n",
    "import matplotlib.pyplot as plt # plots results\n",
    "import matplotlib\n",
    "import re # uses regular expressions to efficiently search large bodies of text\n",
    "import pickle # allows export of dictionaries and facilitates pickle rick jokes \n",
    "from collections import Counter # rapidly parses dictionaries\n",
    "import os \n",
    "import math\n",
    "import time\n",
    "from glob import glob \n",
    "import calendar\n",
    "from datetime import datetime \n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading 1803-2004 \n",
    "\n",
    "The online archives for 1803-2005, 2005-2010, and 2010-2017 are all held in slightly different places and in slightly different formats, presumably because Alan/Colin/Dan/Graham over at the Parliamentary Digital Service isn't too hot on his database design, so we have three different functions to download each of these.\n",
    "\n",
    "We cycle through every possible date in our time range and check whether a page corresponding to that date exists in the Hansard archive. If a page exists, that means there was a Parliamentary sitting that day and we can download the associated records. If a page doesn't exist, we can assume Parliament wasn't sitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveOldContribs():\n",
    "\n",
    "    for year in range(1803,2005):\n",
    "        for month in range(1,13):\n",
    "            for day in range(1,32):\n",
    "\n",
    "                monthName = calendar.month_name[month].lower()[0:3]\n",
    "                contentsURL = \"http://hansard.millbanksystems.com/sittings/\"+str(year)+\"/\"+monthName+\"/\"+str(day).zfill(2)\n",
    "\n",
    "                try: \n",
    "                    print \"trying date: \", str(year)+' '+str(month)+' '+str(day).zfill(2)\n",
    "                    contentsPage = urllib2.urlopen(contentsURL).read()\n",
    "                    contentsSoup = BeautifulSoup(contentsPage,'lxml')\n",
    "                    print \"success!\"\n",
    "                    saveFile = open('./'+str(year)+' '+str(month)+' '+str(day).zfill(2)+'.html', 'w')\n",
    "                    commonsSection = contentsSoup.find_all(\"ol\", {\"class\": \"xoxo first\"})[0]\n",
    "                    for link in commonsSection.find_all('a', href=True):\n",
    "                        linkURL = \"http://hansard.millbanksystems.com\"+link['href']\n",
    "                        print linkURL\n",
    "                        linkPage = urllib2.urlopen(linkURL).read()\n",
    "                        saveFile.write(linkPage)\n",
    "\n",
    "                    saveFile.close()\n",
    "                except:\n",
    "                    print \"page not found!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading 2005-2010\n",
    "\n",
    "Note especially the skillful use of 'sesh' as a variable name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveMidContribs(): \n",
    "\n",
    "    validSessions = ['200405 2004 11','200405 2004 12','200405 2005 01','200405 2005 02','200405 2005 03','200405 2005 04',\n",
    "                     '200506 2005 05','200506 2005 06','200506 2005 07','200506 2005 08','200506 2005 09','200506 2005 10',\n",
    "                     '200506 2005 11','200506 2005 12','200506 2006 01','200506 2006 02','200506 2006 03','200506 2006 04',\n",
    "                     '200506 2006 05','200506 2006 06','200506 2006 07','200506 2006 08','200506 2006 09','200506 2006 10',\n",
    "                     '200506 2006 11','200607 2006 12','200607 2007 01','200607 2007 02','200607 2007 03','200607 2007 04',\n",
    "                     '200607 2007 05','200607 2007 06','200607 2007 07','200607 2007 08','200607 2007 09','200607 2007 10',\n",
    "                     '200708 2007 11','200708 2007 12','200708 2008 01','200708 2008 02','200708 2008 03','200708 2008 04',\n",
    "                     '200708 2008 05','200708 2008 06','200708 2008 07','200708 2008 08','200708 2008 09','200708 2008 10',\n",
    "                     '200708 2008 11','200809 2008 12','200809 2009 01','200809 2009 02','200809 2009 03','200809 2009 04',\n",
    "                     '200809 2009 05','200809 2009 06','200809 2009 07','200809 2009 08','200809 2009 09','200809 2009 10',\n",
    "                     '200809 2009 11','200910 2009 12','200910 2010 01','200910 2010 02','200910 2010 03','200910 2010 04',\n",
    "                     '201011 2010 05','201011 2010 06','201011 2010 07','201011 2010 08','201011 2010 09','201011 2010 10',\n",
    "                     '201011 2010 11','201011 2010 12']\n",
    "\n",
    "    for sesh in validSessions:\n",
    "        for x in range(1,32):\n",
    "            \n",
    "            day = '%02d'%(x)\n",
    "            session, year, month = sesh.split(\" \")\n",
    "\n",
    "            date = year[2:4]+month+day\n",
    "\n",
    "            print \"trying date \",date\n",
    "\n",
    "            try:\n",
    "                currentDate = time.strptime(day+'/'+month+'/'+year,'%d/%m/%Y')\n",
    "\n",
    "                if year[2] == '0':\n",
    "                    dateShort = year[3:4]+month+day\n",
    "                else:\n",
    "                    dateShort = date\n",
    "\n",
    "                if time.strptime('04/05/2006','%d/%m/%Y') < time.strptime(day+'/'+month+'/'+year,'%d/%m/%Y'):\n",
    "                    pageFlag = \"-0001.htm\"\n",
    "                else:\n",
    "                    pageFlag = \"-01.htm\"\n",
    "\n",
    "                if  time.strptime('08/11/2006','%d/%m/%Y') < time.strptime(day+'/'+month+'/'+year,'%d/%m/%Y'):\n",
    "                    volFlag = \"cm\"\n",
    "                else:\n",
    "                    volFlag = \"vo\"\n",
    "\n",
    "                contentsURL = \"https://publications.parliament.uk/pa/cm\"+session+\"/cmhansrd/\"+volFlag+date+\"/debtext/\"+dateShort+pageFlag\n",
    "                contentsPage = urllib2.urlopen(contentsURL).read()\n",
    "                contentsSoup = BeautifulSoup(contentsPage,'lxml')\n",
    "\n",
    "                if contentsSoup.find(\"h1\") is None:\n",
    "                    print \"success!\"\n",
    "                    print \"...\"\n",
    "                    saveFile = open('./'+year+' '+month+' '+day+'.html', 'w')\n",
    "                    for pageNum in range(1,2000):\n",
    "                        print \"trying page \",pageNum\n",
    "                        if time.strptime('04/05/2006','%d/%m/%Y') < time.strptime(day+'/'+month+'/'+year,'%d/%m/%Y'):\n",
    "                            pageFlag = '-'+str(pageNum).zfill(4)+'.htm'\n",
    "                        else:\n",
    "                            pageFlag = '-'+str(pageNum).zfill(2)+'.htm'\n",
    "\n",
    "                        searchURL = \"https://publications.parliament.uk/pa/cm\"+session+\"/cmhansrd/\"+volFlag+date+\"/debtext/\"+dateShort+pageFlag\n",
    "                        searchPage = urllib2.urlopen(searchURL).read()\n",
    "                        searchSoup = BeautifulSoup(searchPage,'lxml')\n",
    "                        if searchSoup.find(\"h1\") is None:\n",
    "                            saveFile.write(searchPage)\n",
    "                        else:\n",
    "                            break\n",
    "                    saveFile.close()\n",
    "                else:\n",
    "                    print \"no debates on this day!\"\n",
    "\n",
    "            except:\n",
    "                print \"not a valid date!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading 2011-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveNewContribs():\n",
    "\n",
    "    for year in range(2011,2018):\n",
    "        for month in range(1,13):\n",
    "            for day in range(1,32):\n",
    "\n",
    "                contentsURL = \"http://hansard.parliament.uk/commons/\"+str(year)+\"-\"+str(month).zfill(2)+\"-\"+str(day).zfill(2)\n",
    "\n",
    "                print \"trying date: \", str(year)+' '+str(month)+' '+str(day).zfill(2)\n",
    "                print contentsURL\n",
    "                contentsPage = urllib2.urlopen(contentsURL).read()\n",
    "                contentsSoup = BeautifulSoup(contentsPage,'lxml')\n",
    "                commonsSection = contentsSoup.find_all(\"li\", {\"class\": \"no-children\"})\n",
    "                if commonsSection:\n",
    "                    print \"success!\"\n",
    "                    saveFile = open('./'+str(year)+' '+str(month)+' '+str(day).zfill(2)+'.html', 'w')\n",
    "                    for listItem in commonsSection:\n",
    "                        linkURL = \"http://hansard.parliament.uk\"+listItem.find('a', href=True)['href']\n",
    "                        print linkURL\n",
    "                        linkPage = urllib2.urlopen(linkURL).read()\n",
    "                        linkSoup = BeautifulSoup(linkPage,'lxml')\n",
    "                        saveFile.write(linkPage)\n",
    "                else:\n",
    "                    print \"no sittings\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Bunch of Dicts\n",
    "\n",
    "Now we have around 20000 HTML files stored locally on our computers under the naming convention 'YYYY M DD' with each file corresponding to Parliamentary sittings on one date. Searching these files for keyword usage is marginally less horrific than accessing each page of the online archives, but still takes hours. \n",
    "\n",
    "A smarter way to store this information is using dicts, or dictionaries. Sometimes they're also called hash tables but for the purposes of subtle innuendo we'll stick with dicts here. The next function produces one dict for each HTML file, containing all unique words in that file together with corresponding word counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDicts():\n",
    "\n",
    "    for htmlFilePath in glob(\"./Archive/*.html\"):\n",
    "\n",
    "        year = htmlFilePath.split(\"/\")[2].split(\" \")[0]\n",
    "        month = htmlFilePath.split(\"/\")[2].split(\" \")[1].zfill(2)\n",
    "        day = htmlFilePath.split(\"/\")[2].split(\" \")[2][0:2]\n",
    "\n",
    "        savePath = \"./Dicts/\"+  year +\" \"+month+\" \"+day+\".p\"\n",
    "        searchText = open(htmlFilePath,'r').read().lower().translate(None, string.punctuation).split(\" \")\n",
    "        wordDict = Counter(searchText)\n",
    "        pickle.dump(wordDict, open(savePath, \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking word usage\n",
    "\n",
    "This is the fun bit. We can now search our local copy of Hansard to track usage of different words over time. This function searches the archives for usage of a search term between a user specific start and end date. It returns a list of dates on which the search term was mentioned, and a corresponding list of counts for each date. \n",
    "\n",
    "Here, startDate and endDate format must be 'dd mm yyyy', and searchTerms must be a list of strings, each of which is a related search term (e.g. searchTerms = ['India','Indians','Indians']). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchContribs(searchTerms,startDate,endDate):\n",
    "\n",
    "    uniqueDates = []\n",
    "    uniqueCounts = []\n",
    "\n",
    "    start = datetime.strptime(startDate, '%d %m %Y')\n",
    "    end = datetime.strptime(endDate, '%d %m %Y')\n",
    "\n",
    "\n",
    "    for dictFilePath in glob(\"./Dicts/*.p\"):\n",
    "\n",
    "        contribDate = datetime.strptime(\" \".join([dictFilePath.split(\"/\")[2].split(\" \")[2][0:2], dictFilePath.split(\"/\")[2].split(\" \")[1].zfill(2), dictFilePath.split(\"/\")[2].split(\" \")[0]]), '%d %m %Y')\n",
    "\n",
    "        if contribDate >= start and contribDate <= end:\n",
    "            clear_output(wait=True)\n",
    "            print \"opening: \", dictFilePath\n",
    "            wordDict = pickle.load( open( dictFilePath, \"rb\" ) )\n",
    "            counts = 0\n",
    "            wordFlag=0\n",
    "            for term1 in searchTerms:\n",
    "                if wordDict[term1]:\n",
    "                    wordFlag = 1\n",
    "\n",
    "            if wordFlag:\n",
    "                uniqueDates.append(contribDate)\n",
    "                for term2 in searchTerms:\n",
    "                    counts = counts + wordDict[term2]\n",
    "                uniqueCounts.append(counts)\n",
    "\n",
    "    return uniqueDates, uniqueCounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the results\n",
    "\n",
    "Lastly, we have a function to visualise our results as a bar chart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotContribs(searchTerms, uniqueDates, uniqueCounts,plotColour):\n",
    "    \n",
    "    matplotlib.rcParams['toolbar'] = 'None'\n",
    "    plt.bar(uniqueDates,uniqueCounts,color=plotColour,width=3)\n",
    "    plt.title(searchTerms[0])\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchTerms = [\"technology\"]\n",
    "startDate = '01 01 1803'\n",
    "endDate = '31 12 2017'\n",
    "dates, counts = searchContribs(searchTerms,startDate,endDate)\n",
    "plotContribs(searchTerms, dates, counts,'#332288',startDate,endDate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
