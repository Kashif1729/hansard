{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hansard Analysis\n",
    "\n",
    "This notebook contains code to analyse Hansard, the official record of every spoken or written contribution made in the Houses of Parliament from 1803 to the present day."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why bother?\n",
    "\n",
    "Looking at changes in political discourse over time is fascinating. We can infer a lot about which issues were prioritised by different governments, how Members of Parliament responded to global events, when emerging technologies were first mentioned in Parliament, and how the tone surrounding different issues has changed.\n",
    "\n",
    "<img src=\"https://assets3.parliament.uk/iv/main-large//ImageVault/Images/id_10860/scope_0/ImageVaultHandler.aspx.jpg\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the archives\n",
    "\n",
    "The online Hansard archive (available at XXX) is an incredibly rich source of information, and is free to access. It's also a bit of a mess; the archives for 1803-2005, 2005-2010, and 2010-2017 are all held in slightly different places and in slightly different formats. The archive offers some basic search functionality, but not at the speeds or level of detail we need to be able to track, for example, usage of the word \"Empire\" over the past 200 years. This means it's faster and more efficient to download a local copy of Hansard and search that, so let's start off by writing some code to automatically download one HTML file for every day's worth of Parliamentary debate. \n",
    "\n",
    "Here, we cycle through every possible date in our time range and check whether a page corresponding to that date exists in the Hansard archive. If a page exists, that means there was a Parliamentary sitting that day and we can download the associated records. If a page doesn't exist, we can assume Parliament wasn't sitting. \n",
    "\n",
    "This is the programmatic equivalent of accessing the Hansard archive in any web browser, navigating to a particular date, and manually downloading all debates listed under the House of Commons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "\n",
    "import urllib2 # requests and grabs information from web pages\n",
    "from bs4 import BeautifulSoup # parses and searches HTML files\n",
    "import matplotlib.pyplot as plt # plots results\n",
    "import re # uses regular expressions to efficiently search large bodies of text\n",
    "import os \n",
    "import math\n",
    "import time\n",
    "from glob import glob \n",
    "import calendar\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading 1803-2004 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save daily Hansard records from 1803-2004 locally as HTML files\n",
    "def saveOldContribs():\n",
    "\n",
    "    for year in range(1803,2005):\n",
    "        for month in range(1,13):\n",
    "            for day in range(1,32):\n",
    "\n",
    "                monthName = calendar.month_name[month].lower()[0:3]\n",
    "                contentsURL = \"http://hansard.millbanksystems.com/sittings/\"+str(year)+\"/\"+monthName+\"/\"+str(day).zfill(2)\n",
    "\n",
    "                try: \n",
    "                    print \"trying date: \", str(year)+' '+str(month)+' '+str(day).zfill(2)\n",
    "                    contentsPage = urllib2.urlopen(contentsURL).read()\n",
    "                    contentsSoup = BeautifulSoup(contentsPage,'lxml')\n",
    "                    print \"success!\"\n",
    "                    saveFile = open('./XXXX'+ ' '+str(year)+' '+str(month)+' '+str(day).zfill(2)+'.html', 'w')\n",
    "                    commonsSection = contentsSoup.find_all(\"ol\", {\"class\": \"xoxo first\"})[0]\n",
    "                    for link in commonsSection.find_all('a', href=True):\n",
    "                        linkURL = \"http://hansard.millbanksystems.com\"+link['href']\n",
    "                        print linkURL\n",
    "                        linkPage = urllib2.urlopen(linkURL).read()\n",
    "                        saveFile.write(linkPage)\n",
    "\n",
    "                    saveFile.close()\n",
    "                except:\n",
    "                    print \"page not found!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading 2005-2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save daily Hansard records from 2005-2010 locally as HTML files\n",
    "def saveMidContribs():\n",
    "\n",
    "    days = ['01', '02', '03', '04', '05', '06', '07' ,'08' ,'09', '10', '11' ,'12', '13', '14' ,'15',\n",
    "            '16', '17' ,'18' ,'19' ,'20' ,'21' ,'22' ,'23', '24' ,'25' ,'26' ,'27', '28' ,'29', '30' ,'31']\n",
    "\n",
    "    validSessions = ['200405 2004 11','200405 2004 12', '200405 2005 01', '200405 2005 02','200405 2005 03','200405 2005 04',\n",
    "                     '200506 2005 05','200506 2005 06','200506 2005 07','200506 2005 08','200506 2005 09','200506 2005 10',\n",
    "                     '200506 2005 11','200506 2005 12',\n",
    "                     '200506 2006 01','200506 2006 02','200506 2006 03','200506 2006 04','200506 2006 05','200506 2006 06',\n",
    "                     '200506 2006 07','200506 2006 08','200506 2006 09','200506 2006 10','200506 2006 11',\n",
    "                     '200607 2006 12','200607 2007 01','200607 2007 02','200607 2007 03','200607 2007 04','200607 2007 05','200607 2007 06',\n",
    "                     '200607 2007 07','200607 2007 08','200607 2007 09','200607 2007 10',\n",
    "                     '200708 2007 11','200708 2007 12',\n",
    "                     '200708 2008 01','200708 2008 02','200708 2008 03','200708 2008 04','200708 2008 05','200708 2008 06',\n",
    "                     '200708 2008 07','200708 2008 08','200708 2008 09','200708 2008 10','200708 2008 11',\n",
    "                     '200809 2008 12',\n",
    "                     '200809 2009 01','200809 2009 02','200809 2009 03','200809 2009 04','200809 2009 05','200809 2009 06',\n",
    "                     '200809 2009 07','200809 2009 08','200809 2009 09','200809 2009 10','200809 2009 11',\n",
    "                     '200910 2009 12',\n",
    "                     '200910 2010 01','200910 2010 02','200910 2010 03','200910 2010 04',\n",
    "                     '201011 2010 05','201011 2010 06','201011 2010 07','201011 2010 08','201011 2010 09','201011 2010 10',\n",
    "                     '201011 2010 11','201011 2010 12'\n",
    "                     ]\n",
    "\n",
    "    for sesh in validSessions:\n",
    "        for day in days:\n",
    "\n",
    "            session, year, month = sesh.split(\" \")\n",
    "\n",
    "            date = year[2:4]+month+day\n",
    "\n",
    "            print \"trying date \",date\n",
    "\n",
    "            try:\n",
    "                currentDate = time.strptime(day+'/'+month+'/'+year,'%d/%m/%Y')\n",
    "\n",
    "                if year[2] == '0':\n",
    "                    dateShort = year[3:4]+month+day\n",
    "                else:\n",
    "                    dateShort = date\n",
    "\n",
    "                if time.strptime('04/05/2006','%d/%m/%Y') < time.strptime(day+'/'+month+'/'+year,'%d/%m/%Y'):\n",
    "                    pageFlag = \"-0001.htm\"\n",
    "                else:\n",
    "                    pageFlag = \"-01.htm\"\n",
    "\n",
    "                if  time.strptime('08/11/2006','%d/%m/%Y') < time.strptime(day+'/'+month+'/'+year,'%d/%m/%Y'):\n",
    "                    volFlag = \"cm\"\n",
    "                else:\n",
    "                    volFlag = \"vo\"\n",
    "\n",
    "                contentsURL = \"https://publications.parliament.uk/pa/cm\"+session+\"/cmhansrd/\"+volFlag+date+\"/debtext/\"+dateShort+pageFlag\n",
    "                contentsPage = urllib2.urlopen(contentsURL).read()\n",
    "                contentsSoup = BeautifulSoup(contentsPage,'lxml')\n",
    "\n",
    "                if contentsSoup.find(\"h1\") is None:\n",
    "                    print \"success!\"\n",
    "                    print \"...\"\n",
    "                    saveFile = open('./'+session+' '+year+' '+month+' '+day+'.html', 'w')\n",
    "                    for pageNum in range(1,2000):\n",
    "                        print \"trying page \",pageNum\n",
    "                        if time.strptime('04/05/2006','%d/%m/%Y') < time.strptime(day+'/'+month+'/'+year,'%d/%m/%Y'):\n",
    "                            pageFlag = '-'+str(pageNum).zfill(4)+'.htm'\n",
    "                        else:\n",
    "                            pageFlag = '-'+str(pageNum).zfill(2)+'.htm'\n",
    "\n",
    "                        searchURL = \"https://publications.parliament.uk/pa/cm\"+session+\"/cmhansrd/\"+volFlag+date+\"/debtext/\"+dateShort+pageFlag\n",
    "                        searchPage = urllib2.urlopen(searchURL).read()\n",
    "                        searchSoup = BeautifulSoup(searchPage,'lxml')\n",
    "                        if searchSoup.find(\"h1\") is None:\n",
    "                            saveFile.write(searchPage)\n",
    "                        else:\n",
    "                            break\n",
    "                    saveFile.close()\n",
    "                else:\n",
    "                    print \"no debates on this day!\"\n",
    "\n",
    "            except:\n",
    "                print \"not a valid date!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading 2011-2017"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
